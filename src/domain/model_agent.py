import os

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate
from langgraph.config import get_stream_writer
from langgraph.types import Command

from src.adapter.vo.ai_chat_model import AiChatResultVO
from src.domain.constant.constant import AgentTypeEnum
from src.domain.model.model import AdapterState, command_update

import datetime
import json
from typing import Dict, List, Any


# å¯¼å…¥æ‰€éœ€æ¨¡å—
# from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_community.tools.file_management import ListDirectoryTool, FileSearchTool, ReadFileTool,WriteFileTool
from langgraph.graph import StateGraph, MessagesState, START, END


# RAGç³»ç»Ÿç›¸å…³å¯¼å…¥
from langchain_chroma import Chroma
# from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from src.llm import LLMFactory, LLMType
from src.utils.embedding_util import EmbeddingUtil

# å®šä¹‰çŠ¶æ€ç±»
class CollaborativeAgentState(MessagesState):
    """åä½œä»£ç†ç³»ç»Ÿçš„çŠ¶æ€ç±»"""
    information_summary: str = ""
    need_more_info: bool = True
    current_working_path: str = "./"  # æ”¹ä¸ºé€šç”¨çš„é»˜è®¤è·¯å¾„
    discovered_paths: List[str] = []  # æ·»åŠ å·²å‘ç°è·¯å¾„åˆ—è¡¨
    path_context: Dict[str, Any] = {}  # æ·»åŠ è·¯å¾„ä¸Šä¸‹æ–‡ä¿¡æ¯
    target_base_path: str = "./"  # æ·»åŠ ç›®æ ‡åŸºç¡€è·¯å¾„ï¼Œç”¨äºå­˜å‚¨ç”¨æˆ·æŒ‡å®šçš„æ ¹è·¯å¾„

# toolå·¥å…·æ¨¡å‹èŠ‚ç‚¹
def tool_tool_node(state):
    """ä½¿ç”¨toolæ¨¡å‹è°ƒç”¨å·¥å…·è·å–ä¿¡æ¯"""
    messages = state["messages"]
    current_path = state.get("current_working_path", "./")
    discovered_paths = state.get("discovered_paths", [])
    path_context = state.get("path_context", {})
    target_base_path = state.get("target_base_path", "./")
    
    try:
        # åœ¨è°ƒç”¨å·¥å…·å‰ï¼Œä¸ºæ¨¡å‹æä¾›å½“å‰è·¯å¾„ä¸Šä¸‹æ–‡
        if messages and messages[-1].type == "human":
            # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«è·¯å¾„æ“ä½œ
            last_content = messages[-1].content.lower()
            
            # åŠ¨æ€æ„å»ºé¡¹ç›®ç»“æ„å»ºè®®
            project_structure_hints = []
            if discovered_paths:
                # åŸºäºå·²å‘ç°è·¯å¾„æ¨æ–­å¸¸è§ç›®å½•æ¨¡å¼
                for path in discovered_paths[-5:]:
                    if "data" in path:
                        project_structure_hints.append(f"{path} (data)")
                    elif "model" in path:
                        project_structure_hints.append(f"{path} (models)")
                    elif "task" in path:
                        project_structure_hints.append(f"{path} (tasks)")
                    elif "example" in path:
                        project_structure_hints.append(f"{path} (examples)")
            
            if not project_structure_hints:
                # å¦‚æœæ²¡æœ‰å‘ç°è·¯å¾„ï¼Œæä¾›é€šç”¨å»ºè®®
                project_structure_hints = [
                    f"{target_base_path}data/ (potential data directory)",
                    f"{target_base_path}src/ (potential source directory)", 
                    f"{target_base_path}models/ (potential models directory)",
                    f"{target_base_path}examples/ (potential examples directory)"
                ]
            
            # æ„å»ºè·¯å¾„ä¸Šä¸‹æ–‡æç¤º
            path_context_msg = f"""
            CRITICAL PATH CONTEXT - MUST FOLLOW:
            - Base Target Path: {target_base_path}
            - Current Working Path: {current_path}
            - Discovered Paths: {discovered_paths[-5:] if discovered_paths else ['None yet']}
            - Suggested Project Structure: {project_structure_hints[:4]}
            
            ğŸ”¥ MANDATORY PATH USAGE RULES:
            1. NEVER use relative paths - ALWAYS use full paths starting with {target_base_path}
            2. Tool call examples:
               âœ… CORRECT: ListDirectoryTool("{target_base_path}fairseq/")
               âœ… CORRECT: ReadFileTool("{target_base_path}fairseq/data/data_utils.py")
               âŒ WRONG: ListDirectoryTool("fairseq/")
               âŒ WRONG: ReadFileTool("fairseq/data/data_utils.py")
            3. If path fails, try these alternatives with FULL PATHS:
               - {target_base_path}
               - {target_base_path}src/
               - {target_base_path}data/
               - {target_base_path}examples/
               - {target_base_path}lib/
            4. Use FileSearchTool to locate files if unsure of exact path
            5. Build on successfully discovered paths: {discovered_paths[-3:] if discovered_paths else ['None']}
            
            REMEMBER: Every tool call must start with {target_base_path} - this is CRITICAL!
            
            USER REQUEST: {messages[-1].content}
            """
            
            # ä¿®æ”¹æœ€åä¸€æ¡æ¶ˆæ¯ä»¥åŒ…å«è·¯å¾„ä¸Šä¸‹æ–‡
            enhanced_message = HumanMessage(content=path_context_msg)
            messages = messages[:-1] + [enhanced_message]
        
        get_stream_writer()({
            "data": AiChatResultVO(text="ğŸ”§ å¼€å§‹è°ƒç”¨å·¥å…·æ¨¡å‹è¿›è¡Œä»£ç åˆ†æ...").model_dump_json(
                exclude_none=True
            )
        })
        print(f"ğŸ”§ [TOOL_NODE] è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(messages)}")
        if messages:
            print(f"ğŸ”§ [TOOL_NODE] æœ€åä¸€æ¡æ¶ˆæ¯ç±»å‹: {messages[-1].type}")
            print(f"ğŸ”§ [TOOL_NODE] æœ€åä¸€æ¡æ¶ˆæ¯é¢„è§ˆ: {messages[-1].content[:150]}...")
        
        result = tool_agent.invoke({"messages": messages}, {"recursion_limit": 200})
        
        # è¯¦ç»†æ˜¾ç¤ºå·¥å…·æ¨¡å‹çš„è¾“å‡º
        print("\n" + "="*60)
        print("ğŸ”§ [TOOL_MODEL_OUTPUT] å·¥å…·æ¨¡å‹å®Œæ•´è¾“å‡º:")
        print("="*60)
        
        if result.get("messages"):
            get_stream_writer()({
                "data": AiChatResultVO(text=f"ğŸ“Š å·¥å…·æ¨¡å‹è¿”å›äº† {len(result['messages'])} æ¡æ¶ˆæ¯").model_dump_json(
                    exclude_none=True
                )
            })
            print(f"ğŸ“Š è¿”å›æ¶ˆæ¯æ•°é‡: {len(result['messages'])}")
            for i, msg in enumerate(result["messages"]):
                if msg.type == "ai" and len(msg.content) > 100:  # åªè¾“å‡ºé‡è¦çš„AIå›å¤
                    get_stream_writer()({
                        "data": AiChatResultVO(text=f"ğŸ”§ å·¥å…·åˆ†æç»“æœ {i+1}: {msg.content[:200]}...").model_dump_json(
                            exclude_none=True
                        )
                    })
                print(f"\n--- æ¶ˆæ¯ {i+1} ---")
                print(f"ç±»å‹: {msg.type}")
                print(f"å†…å®¹é•¿åº¦: {len(msg.content)} å­—ç¬¦")
                print("å†…å®¹:")
                print(msg.content)
                print("-" * 40)
        else:
            get_stream_writer()({
                "data": AiChatResultVO(text="âš ï¸ å·¥å…·æ¨¡å‹æ²¡æœ‰è¿”å›ä»»ä½•æ¶ˆæ¯").model_dump_json(
                    exclude_none=True
                )
            })
            print("âš ï¸ å·¥å…·æ¨¡å‹æ²¡æœ‰è¿”å›ä»»ä½•æ¶ˆæ¯")
        
        print("="*60)
        print("ğŸ”§ [TOOL_MODEL_OUTPUT] è¾“å‡ºç»“æŸ")
        print("="*60 + "\n")
        
        # æ›´æ–°è·¯å¾„çŠ¶æ€
        updated_path = current_path
        updated_discovered = discovered_paths.copy()
        updated_context = path_context.copy()
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–äº†å·¥å…·ç»“æœ
        if result.get("messages"):
            last_message = result["messages"][-1]
            
            # ä»å·¥å…·å“åº”ä¸­æå–è·¯å¾„ä¿¡æ¯
            if hasattr(last_message, 'content'):
                content = last_message.content
                
                # æå–æˆåŠŸè®¿é—®çš„è·¯å¾„
                import re
                successful_paths = re.findall(r'(?:Listed|Reading|Found).*?([./\w-]+/[./\w-]+)', content)
                for path in successful_paths:
                    if path not in updated_discovered:
                        updated_discovered.append(path)
                        get_stream_writer()({
                            "data": AiChatResultVO(text=f"ğŸ” å‘ç°æ–°è·¯å¾„: {path}").model_dump_json(
                                exclude_none=True
                            )
                        })
                        print(f"Discovered new path: {path}")
                
                # æ£€æµ‹è·¯å¾„é”™è¯¯å¹¶æä¾›æ™ºèƒ½æ¢å¤
                error_patterns = [
                    ("no such file or directory", "PATH_ERROR"),
                    ("Error:", "GENERAL_ERROR"),
                    ("could not", "ACCESS_ERROR"),
                    ("permission denied", "PERMISSION_ERROR")
                ]
                
                # ç‰¹åˆ«æ£€æµ‹ReadFileToolé”™è¯¯
                readfile_error = False
                if "no such file or directory" in content.lower() and "readfiletool" in content.lower():
                    readfile_error = True
                    print("Detected ReadFileTool path error - suggesting FileSearchTool workflow")
                
                error_type = None
                for pattern, err_type in error_patterns:
                    if pattern.lower() in content.lower():
                        error_type = err_type
                        break
                
                # å¦‚æœæ£€æµ‹åˆ°ReadFileToolç‰¹å®šé”™è¯¯ï¼Œæä¾›ä¸“é—¨çš„è§£å†³æ–¹æ¡ˆ
                if readfile_error:
                    recovery_message = HumanMessage(content=f"""
                    READFILETOOL ERROR DETECTED! Here's the correct workflow:
                    
                    ğŸ”¥ CRITICAL: ReadFileTool failed because you used an incorrect path.
                    
                    CORRECT READFILETOOL WORKFLOW:
                    1. NEVER guess file paths for ReadFileTool
                    2. ALWAYS use this 2-step process:
                       
                       Step 1: FileSearchTool("filename.py") to find exact location
                       Step 2: ReadFileTool("{target_base_path}" + exact_path_from_step1)
                    
                    EXAMPLE:
                    - FileSearchTool("data_utils.py") returns "fairseq/fairseq/data/data_utils.py"
                    - Then use ReadFileTool("{target_base_path}fairseq/fairseq/data/data_utils.py")
                    
                    PROJECT CONTEXT:
                    - Base Path: {target_base_path}
                    - Many projects have nested directory structures
                    - FileSearchTool reveals the exact nested path structure
                    
                    Try the FileSearchTool + ReadFileTool workflow now!
                    """)
                    
                    result["messages"].append(recovery_message)
                    
                # å¦‚æœæ£€æµ‹åˆ°è·¯å¾„é”™è¯¯ï¼Œæä¾›æ™ºèƒ½è·¯å¾„å»ºè®®
                elif error_type == "PATH_ERROR":
                    print(f"Detected {error_type}: {content[:200]}")
                    
                    # åŠ¨æ€æ„å»ºè·¯å¾„å»ºè®®
                    suggested_paths = [
                        target_base_path,
                        f"{target_base_path}src/",
                        f"{target_base_path}data/", 
                        f"{target_base_path}models/",
                        f"{target_base_path}examples/",
                        f"{target_base_path}lib/",
                        f"{target_base_path}tasks/",
                        f"{target_base_path}utils/"
                    ]
                    
                    # å¦‚æœæœ‰å†å²æˆåŠŸè·¯å¾„ï¼Œä¼˜å…ˆæ¨èç›¸ä¼¼è·¯å¾„
                    if updated_discovered:
                        # åˆ†ææˆåŠŸè·¯å¾„çš„æ¨¡å¼
                        for successful_path in updated_discovered[-3:]:
                            parent_dir = "/".join(successful_path.split("/")[:-1]) + "/"
                            if parent_dir not in suggested_paths:
                                suggested_paths.insert(0, parent_dir)
                        
                        # åŸºäºæˆåŠŸè·¯å¾„æ¨æ–­å…„å¼Ÿç›®å½•
                        last_successful = updated_discovered[-1]
                        path_parts = last_successful.split("/")
                        if len(path_parts) > 2:
                            base_path = "/".join(path_parts[:-2]) + "/"
                            suggested_paths.insert(0, base_path)
                    
                    recovery_message = HumanMessage(content=f"""
                    Path access failed. Let me use systematic path exploration with FULL PATHS:
                    
                    ğŸ”¥ CRITICAL: Use FULL paths starting with {target_base_path}
                    
                    IMMEDIATE ACTIONS:
                    1. Try these specific FULL paths in order:
                       {chr(10).join(f'   - ListDirectoryTool("{path}")' for path in suggested_paths[:4])}
                    
                    2. If still failing, use FileSearchTool("*.py") to find files
                    
                    3. For ReadFileTool, ALWAYS:
                       a) Use FileSearchTool("filename.py") first to get exact path
                       b) Then use ReadFileTool("{target_base_path}" + exact_path_from_search)
                    
                    PROJECT CONTEXT:
                    - Base Path: {target_base_path}
                    - Current Path: {current_path}
                    - Last Successful: {updated_discovered[-3:] if updated_discovered else 'None'}
                    
                    REMEMBER: NEVER use relative paths like "fairseq/data/" - ALWAYS use "{target_base_path}fairseq/data/"
                    
                    Start with the first suggested path above.
                    """)
                    
                    result["messages"].append(recovery_message)
                    
                elif error_type in ["ACCESS_ERROR", "GENERAL_ERROR"]:
                    recovery_message = HumanMessage(content=f"""
                    Access error encountered. Using alternative approach with FULL PATHS:
                    
                    ğŸ”¥ CRITICAL: Always use FULL paths starting with {target_base_path}
                    
                    1. FileSearchTool("dataset.py") - locate dataset files
                    2. FileSearchTool("data") - find data directories  
                    3. ListDirectoryTool("{target_base_path}") - explore base directory
                    4. Check these common project locations with FULL PATHS:
                       - ListDirectoryTool("{target_base_path}src/")
                       - ListDirectoryTool("{target_base_path}data/")
                       - ListDirectoryTool("{target_base_path}examples/")
                    5. For reading files: 
                       a) FileSearchTool("filename.py") to get exact path
                       b) ReadFileTool("{target_base_path}" + exact_path_from_search)
                    
                    Base path: {target_base_path}
                    Current context: {current_path}
                    
                    REMEMBER: NEVER use paths like "src/" - ALWAYS use "{target_base_path}src/"
                    """)
                    result["messages"].append(recovery_message)
        
        return {
            "messages": result["messages"],
            "information_summary": state.get("information_summary", ""),
            "need_more_info": True,
            "current_working_path": updated_path,
            "discovered_paths": updated_discovered,
            "path_context": updated_context,
            "target_base_path": target_base_path
            }
    except Exception as e:
        print(f"Tool node execution error: {e}")
        error_message = AIMessage(content=f"Tool call encountered an error: {str(e)}. Let me try a more systematic approach to explore the codebase.")
        
        # æä¾›åŸºäºå½“å‰è·¯å¾„çŠ¶æ€çš„æ¢å¤å»ºè®®
        recovery_message = HumanMessage(content=f"""
        System error occurred. Using systematic recovery with FULL PATHS:
        
        ğŸ”¥ CRITICAL: Always use FULL paths starting with {target_base_path}
        
        1. Start with: ListDirectoryTool("{target_base_path}")
        2. Then try: ListDirectoryTool("{current_path}")
        3. Use FileSearchTool to locate target files in base directory
        4. Explore common project directories systematically:
           - ListDirectoryTool("{target_base_path}src/")
           - ListDirectoryTool("{target_base_path}data/")
           - ListDirectoryTool("{target_base_path}examples/")
        5. For file reading: 
           a) FileSearchTool("filename.py") to get exact path
           b) ReadFileTool("{target_base_path}" + exact_path_from_search)
        
        Base path: {target_base_path}
        Current working path: {current_path}
        Known paths: {discovered_paths[-3:] if discovered_paths else 'None'}
        
        REMEMBER: Every tool call must include the full base path {target_base_path}
        """)
        
        return {
            "messages": state["messages"] + [error_message, recovery_message],
            "information_summary": state.get("information_summary", ""),
            "need_more_info": True,
            "current_working_path": current_path,
            "discovered_paths": discovered_paths,
            "path_context": path_context,
            "target_base_path": target_base_path
    }

# DeepSeekæ¨ç†æ¨¡å‹èŠ‚ç‚¹
def reason_node(state):
    """ä½¿ç”¨DeepSeekæ¨ç†æ¨¡å‹åˆ†æä¿¡æ¯å¹¶å†³å®šæ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯"""
    messages = state["messages"]
    current_summary = state.get("information_summary", "")
    current_path = state.get("current_working_path", "./")
    discovered_paths = state.get("discovered_paths", [])
    path_context = state.get("path_context", {})
    target_base_path = state.get("target_base_path", "./")
    
    # åŠ¨æ€åˆ†æé¡¹ç›®ç»“æ„
    project_structure_analysis = "Unknown structure"
    if discovered_paths:
        structure_patterns = []
        for path in discovered_paths:
            if "data" in path.lower():
                structure_patterns.append("data-focused")
            if "model" in path.lower():
                structure_patterns.append("model-centric")
            if "src" in path.lower():
                structure_patterns.append("source-organized")
            if "example" in path.lower():
                structure_patterns.append("example-driven")
        project_structure_analysis = ", ".join(set(structure_patterns)) or "general project"
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ŒåŒ…å«è·¯å¾„ä¸Šä¸‹æ–‡
    system_prompt = f"""
    You are a code analysis assistant. Analyze the collected information and make a CLEAR decision.
    
    FOCUS on these THREE aspects:
    1. Data structure & processing pipeline
    2. Dataloader index organization  
    3. Data augmentation techniques
    
    PROJECT CONTEXT:
    - Base Path: {target_base_path}
    - Current Working Path: {current_path}
    - Successfully Discovered Paths: {discovered_paths[-5:] if discovered_paths else ['None']}
    - Project Structure Type: {project_structure_analysis}
    
    Current Information: {current_summary}
    Latest Info: {messages[-1].content}
    
    DECISION RULES:
    - If you have found key dataset/dataloader files and read their content â†’ NEED_MORE_INFO: false
    - If you have code examples showing data loading/processing â†’ NEED_MORE_INFO: false  
    - If tool encountered repeated file access errors â†’ provide specific alternative paths
    - If no dataset-related files found yet â†’ NEED_MORE_INFO: true
    
    PATH-AWARE INSTRUCTIONS:
    When providing instructions, consider the project context:
    - Use discovered paths: {discovered_paths[-3:] if discovered_paths else [target_base_path]}
    - Focus on project's identified structure type: {project_structure_analysis}
    - Suggest file searches in successful directories
    - Build on successful path patterns
    
    RESPONSE FORMAT:
    ANALYSIS: [Brief analysis of what was found]
    NEED_MORE_INFO: [true/false - be decisive]
    PATH_UPDATE: [Suggest new working path if needed, or "None"]
    INSTRUCTIONS: [If true, give ONE specific instruction with full path like "Read file {target_base_path}src/dataset.py"]
    FINAL_ANSWER: [If false, provide comprehensive answer with code details]
    
    Be DECISIVE and SPECIFIC. Use full paths in instructions based on the project structure.
    """
    
    get_stream_writer()({
        "data": AiChatResultVO(text="ğŸ§  å¼€å§‹æ¨ç†åˆ†æï¼Œæ•´ç†ä»£ç åˆ†æç»“æœ...").model_dump_json(
            exclude_none=True
        )
    })
    print(f"ğŸ§  [REASON_NODE] è¾“å…¥æç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
    print(f"ğŸ§  [REASON_NODE] å½“å‰ä¿¡æ¯æ‘˜è¦é•¿åº¦: {len(current_summary)} å­—ç¬¦")
    print(f"ğŸ§  [REASON_NODE] å‘ç°çš„è·¯å¾„æ•°é‡: {len(discovered_paths)}")
    
    # è°ƒç”¨æ¨ç†æ¨¡å‹
    reasoning_result = reason_model.invoke(system_prompt)
    
    # è¯¦ç»†æ˜¾ç¤ºæ¨ç†æ¨¡å‹çš„è¾“å‡º
    get_stream_writer()({
        "data": AiChatResultVO(text=f"ğŸ§  æ¨ç†æ¨¡å‹åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(reasoning_result.content)} å­—ç¬¦çš„åˆ†æç»“æœ").model_dump_json(
            exclude_none=True
        )
    })
    print("\n" + "="*60)
    print("ğŸ§  [REASONING_MODEL_OUTPUT] æ¨ç†æ¨¡å‹å®Œæ•´è¾“å‡º:")
    print("="*60)
    print(f"ğŸ“Š è¾“å‡ºç±»å‹: {type(reasoning_result)}")
    print(f"ğŸ“Š è¾“å‡ºå†…å®¹é•¿åº¦: {len(reasoning_result.content)} å­—ç¬¦")
    print("\n--- æ¨ç†æ¨¡å‹å®Œæ•´åˆ†æ ---")
    print(reasoning_result.content)
    print("-" * 60)
    print("="*60)
    print("ğŸ§  [REASONING_MODEL_OUTPUT] è¾“å‡ºç»“æŸ")
    print("="*60 + "\n")
    
    # è§£æç»“æœ
    analysis = reasoning_result.content
    
    # æå–æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯çš„å†³å®š
    need_more = "NEED_MORE_INFO: true" in analysis
    
    # æå–è·¯å¾„æ›´æ–°å»ºè®®
    updated_path = current_path
    if "PATH_UPDATE:" in analysis:
        path_suggestion = analysis.split("PATH_UPDATE:")[1].split("INSTRUCTIONS:")[0].split("FINAL_ANSWER:")[0].strip()
        if path_suggestion and path_suggestion != "None" and path_suggestion != "":
            updated_path = path_suggestion
            get_stream_writer()({
            "data": AiChatResultVO(text=f"ğŸ“ æ›´æ–°å·¥ä½œè·¯å¾„: {current_path} -> {updated_path}").model_dump_json(
                exclude_none=True
            )
        })
        print(f"Updating working path: {current_path} -> {updated_path}")
    
    # æå–æœ€ç»ˆåˆ†æç»“æœ
    if "FINAL_ANSWER:" in analysis:
        final_answer = analysis.split("FINAL_ANSWER:")[1].strip()
        messages.append(AIMessage(content=final_answer))
    
    # å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œæå–æŒ‡ä»¤
    if need_more and "INSTRUCTIONS:" in analysis:
        instructions_text = analysis.split("INSTRUCTIONS:")[1].split("FINAL_ANSWER:")[0].strip()
        
        # å¢å¼ºæŒ‡ä»¤ï¼Œç¡®ä¿åŒ…å«è·¯å¾„ä¸Šä¸‹æ–‡
        enhanced_instructions = f"""
        {instructions_text}
        
        PATH CONTEXT REMINDER:
        - Base Path: {target_base_path}
        - Current Path: {updated_path}
        - Known Good Paths: {discovered_paths[-3:] if discovered_paths else ['None yet']}
        - Project Type: {project_structure_analysis}
        - If path fails, try similar patterns from successful paths
        """
        
        messages.append(HumanMessage(content=enhanced_instructions))
    
    # æ›´æ–°ä¿¡æ¯æ‘˜è¦
    if "ANALYSIS:" in analysis:
        new_analysis = analysis.split("ANALYSIS:")[1].split("NEED_MORE_INFO:")[0].strip()
        updated_summary = current_summary + "\n\nNew Analysis: " + new_analysis
    else:
        updated_summary = current_summary
    
    return {
        "messages": messages,
        "information_summary": updated_summary,
        "need_more_info": need_more,
        "current_working_path": updated_path,
        "discovered_paths": discovered_paths,
        "path_context": path_context,
        "target_base_path": target_base_path
    }

# å†³ç­–è·¯ç”±èŠ‚ç‚¹
def router(state):
    """æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
    if not state["need_more_info"]:
        return END
    return "reason_node" if state["messages"][-1].type == "ai" else "tool_tool_node"

def create_workflow(memory):
    """åˆ›å»ºå·¥ä½œæµå›¾"""
    workflow = StateGraph(CollaborativeAgentState)
    workflow.add_node("tool_tool_node", tool_tool_node)
    workflow.add_node("reason_node", reason_node)

    # å®šä¹‰è¾¹ç¼˜å…³ç³»
    workflow.add_conditional_edges(START, router)
    workflow.add_conditional_edges("tool_tool_node", router)
    workflow.add_conditional_edges("reason_node", router)

    # ç¼–è¯‘å›¾æ—¶ä¼ å…¥memoryä½œä¸ºcheckpointer
    return workflow.compile(checkpointer=memory)

def extract_dataset_info_from_analysis(messages, summary):
    """ä»åˆ†æç»“æœä¸­æå–æ•°æ®é›†ç›¸å…³ä¿¡æ¯ - åŸºäºä»£ç çš„è¯¦ç»†ç‰ˆæœ¬"""
    
    # æ”¶é›†æ‰€æœ‰çš„åˆ†æå†…å®¹
    all_content = summary + "\n\n"
    for msg in messages:
        if hasattr(msg, 'content'):
            all_content += msg.content + "\n\n"
    
    # å®šä¹‰æ•°æ®é›†ä¿¡æ¯ç»“æ„
    dataset_info = {
        "timestamp": datetime.datetime.now().isoformat(),
        "analysis_source": "codebase_analysis",
        "codebase_structure": extract_codebase_structure(all_content),
        "data_structure": {
            "raw_data_format": extract_raw_data_format_detailed(all_content),
            "preprocessing_pipeline": extract_preprocessing_pipeline_detailed(all_content),
            "processed_data_format": extract_processed_data_format_detailed(all_content),
            "data_transformations": extract_data_transformations_detailed(all_content)
        },
        "dataloader_organization": {
            "indexing_mechanism": extract_indexing_mechanism_detailed(all_content),
            "sample_structure": extract_sample_structure_detailed(all_content),
            "batch_organization": extract_batch_organization_detailed(all_content),
            "data_relationships": extract_data_relationships_detailed(all_content)
        },
        "data_augmentation": {
            "available_techniques": extract_augmentation_techniques_detailed(all_content),
            "implementation_details": extract_augmentation_implementation_detailed(all_content),
            "application_points": extract_augmentation_points_detailed(all_content)
        },
        "key_files_and_code": extract_key_files_and_code(all_content),
        "training_pipeline": extract_training_pipeline_detailed(all_content),
        "custom_dataset_guidelines": extract_custom_dataset_guidelines_detailed(all_content),
        "code_examples": extract_code_examples(all_content)
    }
    
    return dataset_info

def extract_codebase_structure(content):
    """æå–ä»£ç åº“ç»“æ„ä¿¡æ¯"""
    structure = {
        "identified_directories": [],
        "key_files": [],
        "file_patterns": []
    }
    
    # æå–ç›®å½•ä¿¡æ¯
    import re
    dir_patterns = re.findall(r'(?:directory|folder|path).*?[:\s]([a-zA-Z0-9_/.-]+)', content, re.IGNORECASE)
    structure["identified_directories"] = list(set(dir_patterns))
    
    # æå–æ–‡ä»¶ä¿¡æ¯
    file_patterns = re.findall(r'([a-zA-Z0-9_/.-]+\.py)', content)
    structure["key_files"] = list(set(file_patterns))
    
    return structure

def extract_raw_data_format_detailed(content):
    """æå–åŸå§‹æ•°æ®æ ¼å¼ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    formats = []
    
    # ä»å†…å®¹ä¸­æå–å…·ä½“çš„æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„ä¿¡æ¯
    import re
    
    # æŸ¥æ‰¾æ–‡ä»¶æ‰©å±•åå’Œæ ¼å¼æè¿°
    file_extensions = re.findall(r'\.([a-zA-Z0-9]+)', content)
    format_descriptions = re.findall(r'(?:format|file type|data type)[:\s]*([^.\n]+)', content, re.IGNORECASE)
    
    # æŸ¥æ‰¾å…·ä½“çš„æ•°æ®æ–‡ä»¶ç¤ºä¾‹
    data_file_examples = re.findall(r'([a-zA-Z0-9_.-]+\.(txt|csv|json|bin|idx|wav|jpg|png))', content, re.IGNORECASE)
    
    # æŸ¥æ‰¾æ•°æ®æ ¼å¼ç›¸å…³çš„ä»£ç ç‰‡æ®µ
    code_snippets = re.findall(r'```[a-zA-Z]*\n(.*?)\n```', content, re.DOTALL)
    
    if data_file_examples:
        formats.append({
            "type": "identified_file_formats",
            "examples": [f"{example[0]}" for example in data_file_examples[:10]],
            "extensions": list(set([example[1] for example in data_file_examples])),
            "source": "extracted_from_codebase_analysis"
        })
    
    if format_descriptions:
        formats.append({
            "type": "format_descriptions",
            "descriptions": format_descriptions[:5],
            "source": "analysis_content"
        })
    
    return formats if formats else [{"type": "no_specific_format_identified", "note": "Check the full analysis for format details"}]

def extract_preprocessing_pipeline_detailed(content):
    """æå–é¢„å¤„ç†ç®¡é“ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    pipeline = []
    
    import re
    
    # æŸ¥æ‰¾é¢„å¤„ç†ç›¸å…³çš„å‡½æ•°å’Œç±»
    preprocess_functions = re.findall(r'(?:def|class)\s+([a-zA-Z0-9_]*(?:preprocess|transform|load|parse)[a-zA-Z0-9_]*)', content, re.IGNORECASE)
    
    # æŸ¥æ‰¾é¢„å¤„ç†æ­¥éª¤æè¿°
    process_steps = re.findall(r'(?:step|stage|phase)[:\s]*([^.\n]+)', content, re.IGNORECASE)
    
    # æŸ¥æ‰¾å·¥å…·å’Œå‘½ä»¤
    tools_commands = re.findall(r'(?:command|tool|script)[:\s]*([a-zA-Z0-9_.-]+)', content, re.IGNORECASE)
    
    if preprocess_functions:
        pipeline.append({
            "step": "preprocessing_functions",
            "identified_functions": preprocess_functions[:10],
            "description": "Functions and classes related to data preprocessing",
            "source": "code_analysis"
        })
    
    if tools_commands:
        pipeline.append({
            "step": "preprocessing_tools",
            "tools": tools_commands[:5],
            "description": "External tools and commands for data processing",
            "source": "analysis_content"
        })
    
    if process_steps:
        pipeline.append({
            "step": "process_descriptions",
            "steps": process_steps[:5],
            "description": "Described processing steps",
            "source": "analysis_content"
        })
    
    return pipeline if pipeline else [{"step": "check_full_analysis", "note": "Detailed pipeline information available in full analysis"}]

def extract_processed_data_format_detailed(content):
    """æå–å¤„ç†åæ•°æ®æ ¼å¼ - è¯¦ç»†ç‰ˆæœ¬"""
    formats = []
    if ".bin" in content and ".idx" in content:
        formats.append({
            "type": "binary_indexed",
            "files": [".bin (data)", ".idx (index)"],
            "description": "Binary format with index files for efficient loading"
        })
    if "dict." in content:
        formats.append({
            "type": "vocabulary_files",
            "files": ["dict.source.txt", "dict.target.txt"],
            "description": "Vocabulary mappings for token-to-index conversion"
        })
    return formats if formats else [{"type": "unknown", "description": "Format not identified"}]

def extract_data_transformations_detailed(content):
    """æå–æ•°æ®è½¬æ¢ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    transformations = []
    if "padding" in content.lower():
        transformations.append("Sequence padding for batch processing")
    if "eos" in content.lower():
        transformations.append("EOS token handling")
    if "bos" in content.lower():
        transformations.append("BOS token handling")
    return transformations if transformations else ["Transformations not clearly identified"]

def extract_indexing_mechanism_detailed(content):
    """æå–ç´¢å¼•æœºåˆ¶ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    mechanisms = []
    
    import re
    
    # æŸ¥æ‰¾ç´¢å¼•ç›¸å…³çš„ç±»å’Œå‡½æ•°
    index_classes = re.findall(r'(?:class|def)\s+([a-zA-Z0-9_]*(?:Index|Dataset|Loader)[a-zA-Z0-9_]*)', content, re.IGNORECASE)
    
    # æŸ¥æ‰¾__getitem__æ–¹æ³•
    getitem_methods = re.findall(r'def __getitem__\(.*?\):(.*?)(?=def|\Z)', content, re.DOTALL)
    
    if index_classes:
        mechanisms.append({
            "type": "identified_classes",
            "classes": index_classes[:10],
            "description": "Classes related to indexing and data access",
            "source": "code_analysis"
        })
    
    if getitem_methods:
        mechanisms.append({
            "type": "getitem_implementations",
            "count": len(getitem_methods),
            "description": "Found __getitem__ method implementations",
            "sample_code": getitem_methods[0][:200] + "..." if getitem_methods else "",
            "source": "code_analysis"
        })
    
    return mechanisms if mechanisms else [{"type": "check_full_analysis", "note": "Indexing details available in full analysis"}]

def extract_sample_structure_detailed(content):
    """æå–æ ·æœ¬ç»“æ„ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    structure = {}
    
    # æ£€æŸ¥å¸¸è§å­—æ®µæ¨¡å¼
    common_fields = {
        "id": ["id", "index", "identifier", "key"],
        "input": ["input", "data", "features", "x", "source"],
        "target": ["target", "label", "y", "output", "ground_truth"],
        "metadata": ["metadata", "info", "attributes", "properties"]
    }
    
    for field_type, patterns in common_fields.items():
        if any(pattern in content.lower() for pattern in patterns):
            structure[field_type] = {
                "description": f"{field_type.title()} field containing relevant data",
                "common_names": patterns,
                "usage": f"Used for {field_type} in the model pipeline"
            }
    
    # æ£€æŸ¥æ‰¹æ¬¡ç»“æ„
    batch_patterns = ["batch", "collate", "stack", "pad"]
    if any(pattern in content.lower() for pattern in batch_patterns):
        structure["batch_format"] = {
            "description": "Batched data structure for efficient processing",
            "common_operations": ["padding", "stacking", "collation"],
            "shape": "Typically (batch_size, ...)"
        }
    
    return structure if structure else {"structure": "generic_sample", "note": "Standard sample structure with input/output fields"}

def extract_batch_organization_detailed(content):
    """æå–æ‰¹æ¬¡ç»„ç»‡ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    batch_info = {}
    if "collate" in content.lower():
        batch_info["collation"] = "Uses collate function to merge samples into batches"
    if "pad" in content.lower():
        batch_info["padding"] = "Sequences padded to same length within batch"
    if "sort" in content.lower():
        batch_info["sorting"] = "Samples sorted by length for efficiency"
    
    return batch_info if batch_info else {"organization": "unknown"}

def extract_data_relationships_detailed(content):
    """æå–æ•°æ®å…³ç³»ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    relationships = []
    if "source" in content and "target" in content:
        relationships.append("Source-target sequence pairs aligned line by line")
    if "alignment" in content.lower():
        relationships.append("Word-level alignment between source and target")
    return relationships if relationships else ["Relationships not clearly identified"]

def extract_augmentation_techniques_detailed(content):
    """æå–æ•°æ®å¢å¼ºæŠ€æœ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    techniques = []
    
    # å›¾åƒå¢å¼º
    image_aug_patterns = ["rotate", "flip", "crop", "resize", "color", "brightness", "contrast", "blur"]
    if any(pattern in content.lower() for pattern in image_aug_patterns):
        techniques.append({
            "type": "image_augmentation",
            "methods": ["rotation", "flipping", "cropping", "color adjustment"],
            "purpose": "Increase dataset diversity and model robustness"
        })
    
    # æ–‡æœ¬å¢å¼º
    text_aug_patterns = ["synonym", "paraphrase", "backtranslation", "noise", "dropout", "mask"]
    if any(pattern in content.lower() for pattern in text_aug_patterns):
        techniques.append({
            "type": "text_augmentation", 
            "methods": ["synonym replacement", "paraphrasing", "noise injection"],
            "purpose": "Improve text model generalization"
        })
    
    # éŸ³é¢‘å¢å¼º
    audio_aug_patterns = ["pitch", "speed", "noise", "echo", "reverb", "time_stretch"]
    if any(pattern in content.lower() for pattern in audio_aug_patterns):
        techniques.append({
            "type": "audio_augmentation",
            "methods": ["pitch shifting", "speed change", "noise addition"],
            "purpose": "Enhance audio model robustness"
        })
    
    # é€šç”¨å¢å¼º
    general_aug_patterns = ["augment", "transform", "random", "jitter", "perturbation"]
    if any(pattern in content.lower() for pattern in general_aug_patterns):
        techniques.append({
            "type": "general_augmentation",
            "methods": ["random transformations", "data perturbation"],
            "purpose": "General data augmentation techniques"
        })
    
    return techniques if techniques else [{"type": "no_augmentation", "note": "No specific augmentation techniques identified"}]

def extract_augmentation_implementation_detailed(content):
    """æå–æ•°æ®å¢å¼ºå®ç°ç»†èŠ‚ - è¯¦ç»†ç‰ˆæœ¬"""
    implementation = []
    if "compute_mask_indices" in content:
        implementation.append("Mask indices computation for token masking")
    if "noising" in content.lower():
        implementation.append("Noising transforms for data augmentation")
    return implementation if implementation else ["Implementation details not clearly identified"]

def extract_augmentation_points_detailed(content):
    """æå–æ•°æ®å¢å¼ºåº”ç”¨ç‚¹ - è¯¦ç»†ç‰ˆæœ¬"""
    points = []
    if "dataset" in content.lower() and "transform" in content.lower():
        points.append("Dataset level transformations")
    if "batch" in content.lower() and "transform" in content.lower():
        points.append("Batch level transformations")
    return points if points else ["Application points not clearly identified"]

def extract_key_files_and_code(content):
    """æå–å…³é”®æ–‡ä»¶å’Œä»£ç ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
    key_info = {
        "dataset_files": [],
        "training_files": [],
        "config_files": [],
        "code_snippets": [],
        "class_definitions": [],
        "function_definitions": [],
        "import_statements": []
    }
    
    import re
    
    # æå–æ•°æ®é›†ç›¸å…³æ–‡ä»¶
    dataset_files = re.findall(r'([a-zA-Z0-9_/.-]*(?:dataset|data|loader)[a-zA-Z0-9_/.-]*\.py)', content, re.IGNORECASE)
    key_info["dataset_files"] = list(set(dataset_files))
    
    # æå–è®­ç»ƒç›¸å…³æ–‡ä»¶
    training_files = re.findall(r'([a-zA-Z0-9_/.-]*(?:train|trainer|model)[a-zA-Z0-9_/.-]*\.py)', content, re.IGNORECASE)
    key_info["training_files"] = list(set(training_files))
    
    # æå–é…ç½®æ–‡ä»¶
    config_files = re.findall(r'([a-zA-Z0-9_/.-]*(?:config|setting|cfg)[a-zA-Z0-9_/.-]*\.[a-zA-Z]+)', content, re.IGNORECASE)
    key_info["config_files"] = list(set(config_files))
    
    # æå–ç±»å®šä¹‰
    class_definitions = re.findall(r'class\s+([a-zA-Z0-9_]+)(?:\([^)]*\))?:', content)
    key_info["class_definitions"] = list(set(class_definitions))
    
    # æå–å‡½æ•°å®šä¹‰
    function_definitions = re.findall(r'def\s+([a-zA-Z0-9_]+)\s*\(', content)
    key_info["function_definitions"] = list(set(function_definitions))
    
    # æå–importè¯­å¥
    import_statements = re.findall(r'(?:from\s+[\w.]+\s+)?import\s+([\w., ]+)', content)
    key_info["import_statements"] = [imp.strip() for imp in import_statements[:10]]
    
    # æå–ä»£ç ç‰‡æ®µ - å¢åŠ æ›´å¤šç±»å‹
    code_patterns = [
        (r'```(?:python|py)?\n(.*?)\n```', 'code_block'),
        (r'def __getitem__\(.*?\):(.*?)(?=def|\n\n|\Z)', 'getitem_method'),
        (r'def __init__\(.*?\):(.*?)(?=def|\n\n|\Z)', 'init_method'),
        (r'def collate.*?\(.*?\):(.*?)(?=def|\n\n|\Z)', 'collate_function'),
        (r'class.*?Dataset.*?:(.*?)(?=class|\Z)', 'dataset_class')
    ]
    
    for pattern, snippet_type in code_patterns:
        matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
        for i, match in enumerate(matches[:5]):  # é™åˆ¶æ¯ç§ç±»å‹çš„æ•°é‡
            key_info["code_snippets"].append({
                "type": snippet_type,
                "id": f"{snippet_type}_{i+1}",
                "code": match.strip() if isinstance(match, str) else match[0].strip()
            })
    
    return key_info

def extract_training_pipeline_detailed(content):
    """æå–è®­ç»ƒç®¡é“ä¿¡æ¯ - è¯¦ç»†ç‰ˆæœ¬"""
    pipeline = {}
    if "fairseq-train" in content:
        pipeline["training_command"] = "fairseq-train"
    if "batch" in content.lower():
        pipeline["batching"] = "Dynamic batching based on sequence length"
    if "loss" in content.lower():
        pipeline["loss_computation"] = "Cross-entropy loss for sequence prediction"
    
    return pipeline if pipeline else {"pipeline": "not clearly identified"}

def extract_custom_dataset_guidelines_detailed(content):
    """æå–è‡ªå®šä¹‰æ•°æ®é›†æŒ‡å— - è¯¦ç»†ç‰ˆæœ¬"""
    guidelines = {}
    if "fairseq-preprocess" in content:
        guidelines["preprocessing"] = "Use fairseq-preprocess to binarize your data"
    if "source-lang" in content and "target-lang" in content:
        guidelines["language_specification"] = "Specify source and target languages"
    if "dict." in content:
        guidelines["vocabulary"] = "Dictionary files will be generated automatically"
    
    return guidelines if guidelines else {"guidelines": "not clearly identified"}

def extract_code_examples(content):
    """æå–ä»£ç ç¤ºä¾‹"""
    examples = []
    
    import re
    
    # æå–æ‰€æœ‰ä»£ç å—
    code_blocks = re.findall(r'```(?:python|py|bash|shell)?\n(.*?)\n```', content, re.DOTALL)
    
    for i, code in enumerate(code_blocks[:50]):  # é™åˆ¶æ•°é‡
        examples.append({
            "example_id": f"code_block_{i+1}",
            "code": code.strip(),
            "context": "extracted_from_analysis",
            "relevance": "data_processing_or_training"
        })
    
    return examples

class CodeAnalysisAgent:
    """ä»£ç åˆ†æä»£ç†ç±»ï¼ŒåŒ…å«ä¸“ä¸šçš„RAGå†å²è®°å½•ç³»ç»Ÿ"""
    
    def __init__(self, chat_model, reason_model, embedding_model = None, chroma_persist_directory="./chroma_db"):
        """åˆå§‹åŒ–ä»£ç†ï¼ŒåŒ…æ‹¬æ¨¡å‹å’ŒRAGç³»ç»Ÿ"""
        # åˆå§‹åŒ–æ¨¡å‹
        self.tool_model = chat_model#ChatDeepSeek(model="deepseek-chat")
        self.reason_model = reason_model#ChatDeepSeek(model="deepseek-reasoner")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = embedding_model#self._initialize_embeddings()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.chroma_persist_directory = chroma_persist_directory
        self.vectorstore = self._initialize_vectorstore()
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            is_separator_regex=False,
        )
        
        # RAGæ£€ç´¢å™¨
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # RAG chain
        self.rag_chain = self._create_rag_chain()
        
        # å†å²è®°å½•å­˜å‚¨
        self.analysis_history = []
        
        # å·¥å…·æ¨¡å‹ä»£ç†ï¼ˆå°†åœ¨actionä¸­åŠ¨æ€åˆ›å»ºï¼‰
        self.tool_agent = None
        
        print("CodeAnalysisAgent initialized with professional RAG system")
    
    # def _initialize_embeddings(self):
    #     """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨Googleå¤šè¯­è¨€åµŒå…¥"""
    #     # é¦–å…ˆå°è¯•Googleå¤šè¯­è¨€åµŒå…¥æ¨¡å‹
    #     try:
    #         from langchain_mistralai import MistralAIEmbeddings
    #         os.environ["MISTRAL_API_KEY"] = "ajdw9kGwkGNh6QwSdFa1jAlpP7TEr45t"
    #         embeddings = MistralAIEmbeddings(
    #             model="mistral-embed",
    #         )
    #         return embeddings
    #     except Exception as e:
    #         print(f"MistralAIEmbeddings embeddings failed: {e}, trying fallback...")
    #     try:
    #         print("Using local HuggingFace embeddings...")
    #         from langchain_huggingface import HuggingFaceEmbeddings
    #         return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    #     except ImportError:
    #         print("Warning: No embedding model available")
    #         return None
    
    def _initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if self.embeddings is None:
            print("Warning: No embeddings available, using in-memory storage")
            return None
        
        return Chroma(
            persist_directory=self.chroma_persist_directory,
            embedding_function=self.embeddings
        )
    
    def _create_rag_chain(self):
        """åˆ›å»ºRAGé“¾"""
        if self.vectorstore is None:
            return None
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        template = """You are an expert code analysis assistant. Use the following pieces of context from previous analyses to enhance your current analysis.

Context from previous analyses:
{context}

Current Question: {question}

Based on the historical context and current question, provide an enhanced analysis that builds upon previous insights while addressing the specific current requirements.

Enhanced Analysis:"""
        
        prompt = PromptTemplate.from_template(template)
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡çš„å‡½æ•°
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        # åˆ›å»ºRAGé“¾
        rag_chain = (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.reason_model
            | StrOutputParser()
        )
        
        return rag_chain
    
    def _store_analysis_history(self, target_path, prompt, final_answer, analysis_summary, discovered_paths):
        """å­˜å‚¨åˆ†æå†å²è®°å½•åˆ°å‘é‡æ•°æ®åº“"""
        # åˆ›å»ºæ–‡æ¡£
        analysis_content = f"""
        Target Path: {target_path}
        Prompt: {prompt}
        Analysis Summary: {analysis_summary}
        Final Answer: {final_answer}
        Discovered Paths: {', '.join(discovered_paths)}
        Timestamp: {datetime.datetime.now().isoformat()}
        """
        
        # å­˜å‚¨åˆ°å†å²è®°å½•
        history_record = {
            "timestamp": datetime.datetime.now().isoformat(),
            "target_path": target_path,
            "prompt": prompt,
            "final_answer": final_answer,
            "analysis_summary": analysis_summary,
            "discovered_paths": discovered_paths
        }
        
        self.analysis_history.append(history_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†å¤§å°ï¼ˆæœ€å¤šä¿å­˜20æ¡ï¼‰
        if len(self.analysis_history) > 100:
            self.analysis_history = self.analysis_history[-100:]
        
        # å¦‚æœå‘é‡æ•°æ®åº“å¯ç”¨ï¼Œå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if self.vectorstore is not None:
            try:
                # åˆ†å‰²æ–‡æ¡£
                docs = self.text_splitter.create_documents([analysis_content])
                
                # æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata = {
                        "timestamp": datetime.datetime.now().isoformat(),
                        "target_path": target_path,
                        "type": "analysis_history"
                    }
                
                # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                self.vectorstore.add_documents(docs)
                print(f"Stored analysis history in vector store. Total records: {len(self.analysis_history)}")
            except Exception as e:
                print(f"Failed to store in vector database: {e}")
        else:
            print(f"Stored analysis history in memory. Total records: {len(self.analysis_history)}")
    
    def _create_enhanced_prompt(self, original_prompt, target_path):
        """ä½¿ç”¨RAGç³»ç»Ÿåˆ›å»ºå¢å¼ºçš„prompt"""
        if self.rag_chain is None:
            # å¦‚æœRAGä¸å¯ç”¨ï¼Œå›é€€åˆ°ç®€å•çš„å†å²è®°å½•å¢å¼º
            if self.analysis_history:
                history_context = "\n\n--- PREVIOUS ANALYSIS CONTEXT ---\n"
                for record in self.analysis_history[-2:]:  # ä½¿ç”¨æœ€è¿‘çš„2æ¡è®°å½•
                    history_context += f"Previous Analysis (Path: {record['target_path']}):\n"
                    history_context += f"Summary: {record['analysis_summary'][:200]}...\n\n"
                history_context += "--- END CONTEXT ---\n\n"
                return history_context + original_prompt
            return original_prompt
        
        try:
            # ä½¿ç”¨RAGé“¾è·å–å¢å¼ºçš„prompt
            enhanced_analysis = self.rag_chain.invoke(original_prompt)
            
            # ç»„åˆåŸå§‹promptå’Œå¢å¼ºçš„åˆ†æ
            enhanced_prompt = f"""
            Current Analysis Request:
            {original_prompt}
            
            Previous Analysis Context:
            {enhanced_analysis}
            Please provide a comprehensive analysis that builds upon the previous insights while addressing the current specific requirements.
            """
            
            return enhanced_prompt
            
        except Exception as e:
            print(f"RAG enhancement failed: {e}, using original prompt")
            return original_prompt
    
    def action(self, target_path="./", additional_prompt=""):
        """æ‰§è¡Œä»£ç åˆ†æï¼Œé€»è¾‘ä¸åŸmainå‡½æ•°ç›¸åŒï¼Œä½†æ”¯æŒé¢å¤–promptå’Œå†å²è®°å½•"""
        
        # å¤„ç†è·¯å¾„æ ¼å¼
        if not target_path.endswith('/'):
            target_path += '/'
        
        print(f"Starting analysis for path: {target_path}")
        print(f"Additional prompt length: {len(additional_prompt)} characters")
        
        # æ£€æŸ¥ç›®æ ‡è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(target_path):
            print(f"Target directory exists: {os.listdir(target_path)[:10]}")
        else:
            print(f"Warning: Target path {target_path} does not exist, falling back to current directory")
            target_path = "./"
        
        # ä½¿ç”¨å…¨å±€å˜é‡ä»¥ä¿æŒä¸ç°æœ‰ä»£ç å…¼å®¹
        global tool_model, reason_model, tool_agent
        tool_model = self.tool_model
        reason_model = self.reason_model
        
        # å®šä¹‰å·¥å…· - ç§»é™¤root_dirå‚æ•°ï¼Œè®©æ¨¡å‹ä½¿ç”¨å®Œæ•´è·¯å¾„
        tools = [
            ListDirectoryTool(),
            FileSearchTool(),
            ReadFileTool(),
            WriteFileTool(),
        ]
        
        # åˆ›å»ºå†…å­˜ä¿å­˜å™¨
        memory = MemorySaver()
        
        # åˆ›å»ºç³»ç»Ÿæç¤º
        tool_system_message = SystemMessage(content=f"""
        You are a code analysis expert. Your goal is to find and analyze dataset/dataloader code efficiently.
        
        PROJECT CONTEXT:
        - Base Path: {target_path}
        - Task: Analyze data processing pipeline, dataloader implementation, and data format requirements
        
        CRITICAL PATH HANDLING RULES:
        ğŸ”¥ ALWAYS USE FULL PATHS starting with the base path: {target_path}
        ğŸ”¥ NEVER use relative paths like "A/data/" - ALWAYS use "{target_path}/A/data/"
        ğŸ”¥ When in doubt, construct paths as: {target_path} + subdirectory_path
        
        SMART EXPLORATION STRATEGY:
        
        1. **Start with overview**: ListDirectoryTool("{target_path}") to see project structure
        
        2. **Find dataset files quickly**:
           - FileSearchTool("*dataset*.py") in base directory
           - FileSearchTool("*data*.py") in base directory
           - FileSearchTool("*loader*.py") in base directory
           - FileSearchTool("*train*.py") in base directory
        
        3. **Read key files completely**: When you find relevant files, read them fully using EXACT PATHS:
           - FIRST use FileSearchTool to find exact file locations
           - THEN use ReadFileTool with the EXACT path returned by FileSearchTool
           - Example: If FileSearchTool returns "fairseq/fairseq/data/data_utils.py", use ReadFileTool("{target_path}fairseq/fairseq/data/data_utils.py")
           - NEVER guess paths - ALWAYS use FileSearchTool first to get the exact location
        
        4. **Extract code patterns**: Look for:
           - How samples are indexed (def __getitem__)
           - Data transformation pipelines
           - Batch creation (collate_fn)
           - File format handling
        
        5. **Path construction examples**:
           âœ… Correct: ListDirectoryTool("{target_path}fairseq/data/")
           âœ… Correct: FileSearchTool("data_utils.py") â†’ returns "fairseq/fairseq/data/data_utils.py" â†’ use ReadFileTool("{target_path}fairseq/fairseq/data/data_utils.py")
           âŒ Wrong: ListDirectoryTool("fairseq/data/")
           âŒ Wrong: ReadFileTool("fairseq/data/data_utils.py") without first checking FileSearchTool results
           
        ğŸ”¥ CRITICAL FOR READFILETOOL:
           - NEVER guess file paths
           - ALWAYS use FileSearchTool first to get exact file location
           - Use the EXACT path returned by FileSearchTool (may have nested directories)
           - Add {target_path} prefix to the FileSearchTool result
        
        6. **If path fails, try these alternatives**:
           - {target_path}src/
           - {target_path}data/
           - {target_path}examples/
           - {target_path}lib/
           - Use FileSearchTool to locate files when unsure
        
        ANALYSIS DEPTH:
        - Don't just list files - READ and ANALYZE the code
        - Extract specific function/class names
        - Identify data flow patterns
        - Note preprocessing steps and transformations
        - Find batch organization logic
        
        EFFICIENT WORKFLOW:
        ```
        1. ListDirectoryTool("{target_path}") 
        2. FileSearchTool("*dataset*.py") + FileSearchTool("*data*.py")
        3. For each relevant file found:
           a) Note the EXACT path returned by FileSearchTool
           b) Use ReadFileTool("{target_path}" + exact_path_from_search)
           c) Example: FileSearchTool returns "fairseq/fairseq/data/data_utils.py" 
              â†’ Use ReadFileTool("{target_path}fairseq/fairseq/data/data_utils.py")
        4. Extract specific code patterns and class structures
        ```
        
        FOCUS: Find dataset classes, read their implementation, understand data flow.
        BE THOROUGH: Read complete files, not just snippets.
        ADAPT: This project structure may differ from common patterns - be flexible.
        
        REMEMBER: ALWAYS prefix paths with {target_path} - this is CRITICAL for success!
        """)
        
        # åˆ›å»ºtoolå·¥å…·ä»£ç†
        tool_agent = create_react_agent(
            tool_model,
            tools,
            state_modifier=tool_system_message,
            checkpointer=memory
        )
        self.tool_agent = tool_agent
        
        # åˆ›å»ºå·¥ä½œæµ
        graph = create_workflow(memory)
        
        # é…ç½®
        config = {"configurable": {"thread_id": f"collab_agent_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"}}
        
        # æ„å»ºåŸºç¡€prompt
        base_prompt = """You are a code analysis assistant. I need to understand how to apply a new dataset to this machine learning model/framework. Please analyze the codebase to understand: 1) The input data structure and format requirements, 2) The training pipeline and data processing workflow, 3) The dataloader implementation and how it organizes samples, 4) What data preprocessing is required, 5) How to adapt the system for custom datasets. Focus on extracting the data format details, training procedures, and provide guidance on using custom data with this framework."""
        
        # å¦‚æœæœ‰é¢å¤–promptï¼Œæ·»åŠ åˆ°å‰é¢
        if additional_prompt.strip():
            enhanced_base_prompt = additional_prompt.strip() + "\n\n" + base_prompt
        else:
            enhanced_base_prompt = base_prompt
        
        # ä½¿ç”¨RAGç³»ç»Ÿå¢å¼ºprompt
        print(f"ğŸ§  [RAG_ENHANCEMENT] å¼€å§‹ä½¿ç”¨RAGç³»ç»Ÿå¢å¼ºprompt...")
        print(f"ğŸ§  [RAG_ENHANCEMENT] åŸå§‹prompté•¿åº¦: {len(enhanced_base_prompt)} å­—ç¬¦")
        print(f"ğŸ§  [RAG_ENHANCEMENT] å†å²è®°å½•æ•°é‡: {len(self.analysis_history)}")
        
        final_prompt = self._create_enhanced_prompt(enhanced_base_prompt, target_path)
        
        print(f"ğŸ§  [RAG_ENHANCEMENT] å¢å¼ºåprompté•¿åº¦: {len(final_prompt)} å­—ç¬¦")
        print(f"ğŸ§  [RAG_ENHANCEMENT] é•¿åº¦å¢åŠ : {len(final_prompt) - len(enhanced_base_prompt)} å­—ç¬¦")
        
        if len(final_prompt) > len(enhanced_base_prompt):
            print("âœ… [RAG_ENHANCEMENT] RAGç³»ç»ŸæˆåŠŸå¢å¼ºäº†prompt")
        else:
            print("âš ï¸ [RAG_ENHANCEMENT] RAGç³»ç»Ÿæœªèƒ½å¢å¼ºpromptï¼Œä½¿ç”¨åŸå§‹prompt")
        
        # æ‰§è¡Œåˆ†æ
        get_stream_writer()({
            "data": AiChatResultVO(text=f"ğŸš€ å¯åŠ¨åä½œä»£ç†ç³»ç»Ÿåˆ†æè·¯å¾„: {target_path}").model_dump_json(
                exclude_none=True
            )
        })
        print(f"ğŸš€ åˆå§‹æç¤ºè¯é•¿åº¦: {len(final_prompt)} å­—ç¬¦")
        print(f"ğŸš€ ç›®æ ‡è·¯å¾„: {target_path}")
        print(f"ğŸš€ é…ç½®: {config}")
        final_state = None
        step_count = 0
        
        try:
            for step in graph.stream(
                {
                    "messages": [HumanMessage(content=final_prompt)],
                    "information_summary": "",
                    "need_more_info": True,
                    "current_working_path": target_path,
                    "discovered_paths": [],
                    "path_context": {},
                    "target_base_path": target_path
                },
                config,
                stream_mode="values",
            ):
                step_count += 1
                get_stream_writer()({
                    "data": AiChatResultVO(text=f"ğŸ”„ æ­¥éª¤ {step_count}: åˆ†æè¿›è¡Œä¸­... (å‘ç° {len(step.get('discovered_paths', []))} ä¸ªè·¯å¾„)").model_dump_json(
                        exclude_none=True
                    )
                })
                print(f"\n{'='*50}")
                print(f"ğŸ”„ [STEP {step_count}] Current node: {step.get('node_name', 'Unknown')}")
                print(f"ğŸ”„ [STEP {step_count}] Need more info: {step.get('need_more_info', 'Unknown')}")
                print(f"ğŸ”„ [STEP {step_count}] Current path: {step.get('current_working_path', 'Unknown')}")
                print(f"ğŸ”„ [STEP {step_count}] Discovered paths: {len(step.get('discovered_paths', []))}")
                
                if step.get("messages") and len(step["messages"]) > 0:
                    print(f"ğŸ”„ [STEP {step_count}] Messages count: {len(step['messages'])}")
                    
                    # æ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
                    for i, msg in enumerate(step["messages"]):
                        msg_preview = msg.content[:150] + "..." if len(msg.content) > 150 else msg.content
                        print(f"    Message {i+1} ({msg.type}): {msg_preview}")
                    
                    latest_msg = step["messages"][-1]
                    print(f"ğŸ”„ [STEP {step_count}] Latest message type: {latest_msg.type}")
                    print(f"ğŸ”„ [STEP {step_count}] Latest message length: {len(latest_msg.content)} å­—ç¬¦")
                
                print(f"{'='*50}")
                final_state = step
            
            get_stream_writer()({
                "data": AiChatResultVO(text=f"ğŸ‰ åˆ†æå®Œæˆ! æ€»å…±æ‰§è¡Œäº† {step_count} ä¸ªæ­¥éª¤").model_dump_json(
                    exclude_none=True
                )
            })
            print(f"\nğŸ‰ Execution completed! Total steps: {step_count}")
            
            # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯
            if final_state:
                print("\n" + "="*60)
                print("ğŸ“‹ [FINAL_STATE] æœ€ç»ˆçŠ¶æ€è¯¦æƒ…:")
                print("="*60)
                print(f"ğŸ“Š æœ€ç»ˆæ¶ˆæ¯æ•°é‡: {len(final_state.get('messages', []))}")
                print(f"ğŸ“Š ä¿¡æ¯æ‘˜è¦é•¿åº¦: {len(final_state.get('information_summary', ''))}")
                print(f"ğŸ“Š å‘ç°çš„è·¯å¾„: {final_state.get('discovered_paths', [])}")
                print(f"ğŸ“Š æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯: {final_state.get('need_more_info', 'Unknown')}")
                print(f"ğŸ“Š å½“å‰å·¥ä½œè·¯å¾„: {final_state.get('current_working_path', 'Unknown')}")
                
                # æ˜¾ç¤ºæ‰€æœ‰æœ€ç»ˆæ¶ˆæ¯
                final_messages = final_state.get("messages", [])
                if final_messages:
                    print(f"\n--- æœ€ç»ˆå¯¹è¯å†å² ({len(final_messages)} æ¡æ¶ˆæ¯) ---")
                    for i, msg in enumerate(final_messages):
                        print(f"\næ¶ˆæ¯ {i+1} ({msg.type}):")
                        print(f"é•¿åº¦: {len(msg.content)} å­—ç¬¦")
                        print("å†…å®¹:")
                        print(msg.content)
                        print("-" * 40)
                
                print("="*60)
                print("ğŸ“‹ [FINAL_STATE] çŠ¶æ€è¯¦æƒ…ç»“æŸ")
                print("="*60 + "\n")
            
            # å¤„ç†ç»“æœ
            if final_state:
                final_messages = final_state.get("messages", [])
                final_summary = final_state.get("information_summary", "")
                discovered_paths = final_state.get("discovered_paths", [])
                
                final_answer = final_messages[-1].content if final_messages else "No final answer generated"
                
                # å­˜å‚¨åˆ°å†å²è®°å½•
                self._store_analysis_history(
                    target_path=target_path,
                    prompt=enhanced_base_prompt,
                    final_answer=final_answer,
                    analysis_summary=final_summary,
                    discovered_paths=discovered_paths
                )
                
                # æ„å»ºè¾“å‡ºæ•°æ®
                output_data = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "target_base_path": target_path,
                    "discovered_paths": discovered_paths,
                    "analysis_summary": final_summary,
                    "final_answer": final_answer,
                    "additional_prompt_used": additional_prompt,
                    "conversation_history": [
                        {
                            "role": msg.type,
                            "content": msg.content
                        }
                        for msg in final_messages
                    ]
                }
                
                # ä¿å­˜ç»“æœæ–‡ä»¶
                project_name = os.path.basename(os.path.abspath(target_path.rstrip('/'))) or "project"
                output_file = f"{project_name}_analysis_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=2)
                
                get_stream_writer()({
                    "data": AiChatResultVO(text=f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}").model_dump_json(
                        exclude_none=True
                    )
                })
                print(f"Analysis results saved to file: {output_file}")
                
                # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
                dataset_info = extract_dataset_info_from_analysis(final_messages, final_summary)
                dataset_info["target_base_path"] = target_path
                dataset_info["discovered_paths"] = discovered_paths
                dataset_info["additional_prompt_used"] = additional_prompt
                
                dataset_info_file = f"{project_name}_dataset_info_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(dataset_info_file, "w", encoding="utf-8") as f:
                    json.dump(dataset_info, f, ensure_ascii=False, indent=2)
                
                get_stream_writer()({
                    "data": AiChatResultVO(text=f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°æ–‡ä»¶: {dataset_info_file}").model_dump_json(
                        exclude_none=True
                    )
                })
                print(f"Dataset-specific information saved to file: {dataset_info_file}")
                
                # ç”ŸæˆMarkdownæŠ¥å‘Š
                markdown_content = f"""# {project_name.title()} Analysis Report
Generation Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Target Path: {target_path}
Additional Prompt: {additional_prompt[:1000] + '...' if len(additional_prompt) > 1000 else additional_prompt}
Discovered Paths: {', '.join(discovered_paths[:5])}
Historical Context Used: {'Yes' if len(self.analysis_history) > 1 else 'No'}

## Analysis Summary
{final_summary}

## Final Conclusion
{final_answer}
"""
                
                markdown_file = f"{project_name}_analysis_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                with open(markdown_file, "w", encoding="utf-8") as f:
                    f.write(markdown_content)
                
                get_stream_writer()({
                    "data": AiChatResultVO(text=f"ğŸ“ ç®€æ´æŠ¥å‘Šå·²ä¿å­˜åˆ°æ–‡ä»¶: {markdown_file}").model_dump_json(
                        exclude_none=True
                    )
                })
                print(f"Concise report saved to file: {markdown_file}")
                
                # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹
                file_contents = {}
                try:
                    with open(output_file, "r", encoding="utf-8") as f:
                        file_contents["analysis_json"] = json.load(f)
                except Exception as e:
                    print(f"Failed to read {output_file}: {e}")
                    file_contents["analysis_json"] = None
                
                try:
                    with open(dataset_info_file, "r", encoding="utf-8") as f:
                        file_contents["dataset_info_json"] = json.load(f)
                except Exception as e:
                    print(f"Failed to read {dataset_info_file}: {e}")
                    file_contents["dataset_info_json"] = None
                
                try:
                    with open(markdown_file, "r", encoding="utf-8") as f:
                        file_contents["markdown_report"] = f.read()
                except Exception as e:
                    print(f"Failed to read {markdown_file}: {e}")
                    file_contents["markdown_report"] = None
                
                return {
                    "success": True,
                    "final_answer": final_answer,
                    "analysis_summary": final_summary,
                    "discovered_paths": discovered_paths,
                    "output_files": [output_file, dataset_info_file, markdown_file],
                    "file_contents": file_contents,
                    "history_records_count": len(self.analysis_history),
                    "markdown":markdown_content,
                    "json_out":dataset_info,
                    "summary":output_data
                }
            else:
                return {
                    "success": False,
                    "error": "No final state generated",
                    "history_records_count": len(self.analysis_history)
                }
                
        except Exception as e:
            get_stream_writer()({
                "data": AiChatResultVO(text=f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}").model_dump_json(
                    exclude_none=True
                )
            })
            print(f"Error during analysis: {e}")
            return {
                "success": False,
                "error": str(e),
                "history_records_count": len(self.analysis_history)
            }
    
    def get_history_summary(self):
        """è·å–å†å²è®°å½•æ‘˜è¦"""
        if not self.analysis_history:
            return "No historical analysis records available."
        
        summary = f"Historical Analysis Records: {len(self.analysis_history)} total\n\n"
        
        for i, record in enumerate(self.analysis_history[-20:], 1):  # æ˜¾ç¤ºæœ€è¿‘5æ¡
            summary += f"{i}. {record['timestamp'][:19]}\n"
            summary += f"   Path: {record['target_path']}\n"
            summary += f"   Summary: {record['analysis_summary'][:100]}...\n\n"
        
        return summary
    
    def clear_history(self):
        """æ¸…é™¤å†å²è®°å½•"""
        self.analysis_history.clear()
        
        # æ¸…é™¤å‘é‡æ•°æ®åº“
        if self.vectorstore is not None:
            try:
                # é‡æ–°åˆå§‹åŒ–å‘é‡æ•°æ®åº“
                import shutil
                if os.path.exists(self.chroma_persist_directory):
                    shutil.rmtree(self.chroma_persist_directory)
                self.vectorstore = self._initialize_vectorstore()
                self.retriever = self.vectorstore.as_retriever(
                    search_type="similarity", 
                    search_kwargs={"k": 3}
                ) if self.vectorstore else None
                self.rag_chain = self._create_rag_chain()
                print("Analysis history and vector database cleared.")
            except Exception as e:
                print(f"Failed to clear vector database: {e}")
        else:
            print("Analysis history cleared.")
    
    def search_history(self, query, top_k=3):
        """æœç´¢å†å²è®°å½•"""
        if self.vectorstore is None:
            # ç®€å•çš„æ–‡æœ¬æœç´¢å›é€€
            results = []
            query_lower = query.lower()
            for record in self.analysis_history:
                content = f"{record['prompt']} {record['analysis_summary']} {record['final_answer']}".lower()
                if query_lower in content:
                    results.append(record)
            return results[:top_k]
        
        try:
            # ä½¿ç”¨å‘é‡æœç´¢
            docs = self.vectorstore.similarity_search(query, k=top_k)
            return [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
        except Exception as e:
            print(f"Vector search failed: {e}")
            return []
    
    def get_rag_stats(self):
        """è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "memory_records": len(self.analysis_history),
            "vector_store_available": self.vectorstore is not None,
            "embeddings_available": self.embeddings is not None,
            "rag_chain_available": self.rag_chain is not None,
            "persist_directory": self.chroma_persist_directory
        }
        
        if self.vectorstore is not None:
            try:
                # å°è¯•è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
                collection = self.vectorstore._collection
                stats["vector_store_count"] = collection.count()
            except Exception as e:
                stats["vector_store_count"] = "unknown"
                stats["vector_store_error"] = str(e)
        
        return stats





class ModelAgent:
    def __init__(self):
        self.agent = CodeAnalysisAgent(chat_model=LLMFactory.create_llm(LLMType.DEEPSEEK_CHAT),reason_model=LLMFactory.create_llm(LLMType.DEEPSEEK_REASON),embedding_model=EmbeddingUtil())

    async def __call__(self, state: AdapterState) -> Command:
        try:
            path = state['model_path']
        except KeyError:
            get_stream_writer()({
                "data": AiChatResultVO(text="âŒ No model path provided").model_dump_json(
                    exclude_none=True
                )
            })
            return Command(
                goto=AgentTypeEnum.Supervisor.value,
                update=await command_update(state),
            )

        prompt = state['model_agent_prompt'][-1]
        output = None
        for i in range(5):
            output = self.agent.action(path, prompt)
            if output['success']:
                break
            else:
                pass
        if output['success']:
            state['model_analyse'].append({'markdown':output['markdown'],"json_out":output['json_out'],
                    "summary":output['summary']})
            get_stream_writer()({
                "data": AiChatResultVO(text="âœ…resolve model successful").model_dump_json(
                            exclude_none=True
                        )
            })
        else:
            get_stream_writer()({
                        "data": AiChatResultVO(text="Can't resolve model").model_dump_json(
                            exclude_none=True
                        )
                    })
        return Command(
            goto=AgentTypeEnum.Supervisor.value,
            update=await command_update(state),
        )
